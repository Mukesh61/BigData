"""
One important point to remember is that, decimal128 in pyarrow is equivalent to decimal.Decimal in python and numeric(18,9) or numeric(9,6) or any numeric datatype in redshift
so don't forget to add that typecast in pandas dataframe itself otherwise you will get bytecode error expected 16 got 8 or 2 
"""
import s3fs
import pandas as pd
import os
import site
import importlib
import numpy as np
import decimal
from setuptools.command import easy_install
install_path = os.environ['GLUE_INSTALLATION']

libraries = ["pyarrow"]
for lib in libraries:
    easy_install.main( ["--install-dir", install_path, lib] )

importlib.reload(site)
import pyarrow as pa
import pyarrow.parquet as pq



from s3fs import S3FileSystem
s3 = S3FileSystem()
output_file = "s3://bucket_name/Output/test_aws.parquet"
output_file1 = "s3://bucket_name/Refined/test_aws"
dataset = pq.ParquetDataset(output_file,filesystem =s3)

df = dataset.read_pandas().to_pandas()
print(df)
# decimal128 conversion is byte code 16 conversion and for that we need to first typecast our dataframe into decimal.Decimal which is similar to decimal128
df['D']=df['D'].astype(str).map(decimal.Decimal)
print("I am here")
fields = [pa.field('A', pa.int64()),pa.field('B', pa.int64()),pa.field('C', pa.int64()),pa.field('D', pa.decimal128(precision=9,scale=6))]

my_schema = pa.schema(fields)
print("very good")

table = pa.Table.from_pandas(df,schema=my_schema, preserve_index=False)
print(table)
print("table creation is done, Boss!")
pq.write_to_dataset(table=table,root_path=output_file1,filesystem=s3) 
print("file creation is done, Boss!")
