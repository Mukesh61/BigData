"""
one important point here, if you need to create partition dataset then use write_to_dataset you will find example in some other file.
However if you want to create a file without partition then use below method.
we need to follow some step here.

1. create a parquet writer object 
2. use writer object to write parquet file using write_table
   best part is, it override to existing file if there is any. so in case of rerun of process. we don't have to delete any file which get created in process.
   
3. use filesystem as s3 otherwise you will get error in glue job like "No file/directory presennt" 

"""
import s3fs
import pandas as pd
import os
import site
import importlib
import numpy as np
import decimal
from setuptools.command import easy_install
install_path = os.environ['GLUE_INSTALLATION']

libraries = ["pyarrow"]
for lib in libraries:
    easy_install.main( ["--install-dir", install_path, lib] )

importlib.reload(site)
import pyarrow as pa
import pyarrow.parquet as pq
from s3fs import S3FileSystem

s3 = S3FileSystem()
output_file = "s3://test-pub-redshift/Output/test_aws.parquet"
output_file1 = "s3://test-pub-redshift/Refined/test_aws.parquet"
dataset = pq.ParquetDataset(output_file,filesystem =s3)

df = dataset.read_pandas().to_pandas()
print(df)
df['D']=df['D'].astype(str).map(decimal.Decimal)
print("I am here")
fields = [pa.field('A', pa.int64()),pa.field('B', pa.int64()),pa.field('C', pa.int64()),pa.field('D', pa.decimal128(precision=9,scale=6))]

my_schema = pa.schema(fields)
print("very good")

table = pa.Table.from_pandas(df,schema=my_schema, preserve_index=False)
writer = pq.ParquetWriter(output_file1, table.schema,filesystem=s3)
writer.write_table(table)
#print(table)
writer.close()
print("table creation is done, Boss!")
